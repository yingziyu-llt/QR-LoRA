# ğŸš€ QR-LoRA ğŸš€

ğŸ‰ **Accepted to ICCV 2025**

**Official PyTorch implementation of QR-LoRA**

[QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation](https://arxiv.org/abs/2507.04599v2)  

##  ğŸŒ¼ Abstract
We propose QR-LoRA, a novel fine-tuning framework leveraging QR decomposition for structured parameter updates that effectively separate visual attributes. Our key insight is that the orthogonal Q matrix naturally minimizes interference between different visual features, while the upper triangular R matrix efficiently encodes attribute-specific transformations. Our approach fixes both Q and R matrices while only training an additional task-specific Î”R matrix. This structured design reduces trainable parameters to half of conventional LoRA methods and supports effective merging of multiple adaptations without cross-contamination due to the strong disentanglement properties between Î”R matrices.

![QR-LoRA](assets/qr-method.png)


## ğŸ¯ Key Features in QR-LoRA

- **ğŸ”„ Superior Disentanglement**: Achieves superior disentanglement in content-style fusion tasks through orthogonal decomposition
- **âš¡ Parameter Efficiency**: Reduces trainable parameters to half of conventional LoRA methods
- **ğŸ”§ Easy Integration**: Simple element-wise addition for merging multiple adaptations without cross-contamination
- **ğŸš€ Fast Convergence**: Enhanced initialization strategy enables faster convergence when training both Q and R matrices
- **ğŸ¨ Flexible Control**: Fine-grained control over content and style features through scaling coefficients

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/luna-ai-lab/QR-LoRA.git
cd QR-LoRA
conda create -n qrlora python=3.10 -y
conda activate qrlora
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### Train QR-LoRA for a given input image based on [FLUX-dev1](https://huggingface.co/black-forest-labs/FLUX.1-dev):

```bash
# for style:
bash flux_dir/train_deltaR_sty.sh 0 64

# for content:
bash flux_dir/train_deltaR_cnt.sh 0 64

# for fast convergence in one task:
bash flux_dir/train_QR.sh 0 64
```

#### Inference:
To reduce inference time overhead, we recommend pre-saving the initialization decomposition matrices. This eliminates the need to perform initialization decomposition during each inference:
```bash
bash flux_dir/save_flux_residual.sh 1
```
After executing the above script, a model file `flux_residual_weights.safetensors` will be generated in the `flux_dir` directory. Alternatively, you can also download it directly from [ğŸ¤—flux_res](https://huggingface.co/yjh001/flux_res). Then, configure the corresponding training model path and execute the inference script:
```bash
bash flux_dir/inference_merge.sh 1
```

#### Similarity analysis:
```bash
bash test/visualize_qrlora_similarity.sh 1
```


## TODO
- [ ] Add SDXL-based QR-LoRA training and inference scripts
- [ ] Release pre-trained QR-LoRA model weights
- [ ] Provide more tutorials and application cases


## Limitations
Disentanglement is not a sufficient and necessary condition for good LoRA merging. While good merging results can imply disentanglement properties, having disentanglement properties does not always guarantee good merging performance.


## ğŸ¤ Acknowledgements
We are deeply thankful to the authors of the following works, which have significantly inspired our ideas and methodologies. 

- [ZipLoRA](https://ziplora.github.io/)

- [B-LoRA](https://b-lora.github.io/B-LoRA/)

- [PiSSA](https://arxiv.org/abs/2404.02948)

- [HydraLoRA](https://github.com/Clin0212/HydraLoRA)

We also gratefully acknowledge the open-source libraries like [diffusers](https://huggingface.co/docs/diffusers/index), [transformers](https://huggingface.co/docs/transformers/index), and [accelerate](https://huggingface.co/docs/accelerate/index) that made our research possible.

**We hope that the elegant simplicity of our QR-LoRA approach will inspire further research in your domain!**



## ğŸ“„ Citation
```
@inproceedings{yang2025qrlora,
  title={QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation},
  author={Jiahui Yang and Yongjia Ma and Donglin Di and Hao Li and Wei Chen and Yan Xie and Jianxun Cui and Xun Yang and Wangmeng Zuo},
  booktitle=International Conference on Computer Vision,
  year={2025}
}
```


# å¤§ä½œä¸šä¿®æ”¹ï¼š

åŸä½œè€…åªæä¾›äº† FLUX çš„è„šæœ¬ï¼Œåªèƒ½ç‹¬å ä¸€æ•´å¼  A100 æ‰èƒ½å‹‰å¼ºè·‘å¾—èµ·æ¥ã€‚ä¸ºäº†è¯•éªŒçš„ä¾¿åˆ©ï¼Œæˆ‘ä»¬é€‰æ‹©å°†ä»£ç è¿ç§»åˆ° SD3 ä¸Šã€‚è®­ç»ƒè„šæœ¬ä½¿ç”¨æ–¹æ³•ä¸åŸå§‹è®ºæ–‡ä¸€è‡´ã€‚å…¶ä¸­ï¼Œ `*_ica_*` æ˜¯ ICA-LoRA ç›¸å…³è„šæœ¬ï¼Œ  `*_moe_*` æ˜¯ Freq_MoE ç›¸å…³è„šæœ¬ã€‚

æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”±äº ICA æ¯æ¬¡è§£å‡ºçš„ç»“æœå…·æœ‰éšæœºæ€§ï¼Œæ•…éœ€è¦ç”¨ `SD3_dir/extract_residual_from_checkpoint.py` ä»¥æå–ä½™é¡¹ã€‚ä½¿ç”¨æ–¹æ³•ï¼š

```bash
python SD3_dir/extract_residual_from_checkpoint.py --model_path ./SD3 \
       --lora_path ./exps_SD3/your_lora/pytorch_lora_weights.safetensors \
       --output_dir sd3_residuals_exact \
       --device cuda:0 \
```

æ¨ç†ï¼š
```bash
bash  SD3_dir/inference_ica.sh
```